{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgyu2GBVW192"
      },
      "source": [
        "# Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1Qj5KY79W192"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHJ17JDiliHA"
      },
      "source": [
        "### CNN model in Pytorch\n",
        "\n",
        "There are several ways to write a CNN model in pytorch. In this lab, you will be using the _Sequential_ class of pytorch (similarly to Tensorflow). We will see the syntax further on.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os \n",
        "from PIL import Image \n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class SkinDataset(Dataset):\n",
        "    def __init__(self, image_dir, y, transform=None):\n",
        "        # super(SkinDataset, self).__init__()\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.y = y\n",
        "        self.images = os.listdir(image_dir)\n",
        "        self.idx = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        if self.y is not None:\n",
        "            y = np.array(self.y)\n",
        "        if self.transform is not None:\n",
        "            augmentations = self.transform(image=image)\n",
        "            image = augmentations[\"image\"]\n",
        "            if self.y is not None:\n",
        "                return image, y\n",
        "            else:\n",
        "                return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch \n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader \n",
        "import numpy as np \n",
        "\n",
        "\n",
        "def get_loaders(\n",
        "    train_dir,\n",
        "    y_train,\n",
        "    val_dir,\n",
        "    y_val,\n",
        "    test_dir,\n",
        "    y_test,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    test_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    train_ds = SkinDataset(image_dir=train_dir, transform=train_transform,y = y_train)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True,)\n",
        "\n",
        "    val_ds = SkinDataset(image_dir=val_dir, transform=val_transform,y = y_val)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=False,)\n",
        "\n",
        "    test_ds = SkinDataset(image_dir=test_dir, transform=test_transform,y = y_test)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True,)\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "def get_test_loaders(\n",
        "    test_img_dir,\n",
        "    batch_size,\n",
        "    test_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "):\n",
        "    test_ds = SkinDataset(image_dir=test_img_dir, y=None, transform=test_transform,)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory, shuffle=True,)\n",
        "    return test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "\n",
        "Train_directory = 'data/Train_CNN/Train/'\n",
        "if not os.path.exists(Train_directory):\n",
        "    os.mkdir(Train_directory)\n",
        "Val_directory = 'data/Train_CNN/Val/'\n",
        "if not os.path.exists(Val_directory):\n",
        "    os.mkdir(Val_directory)\n",
        "Test_directory = 'data/Train_CNN/Test/'\n",
        "if not os.path.exists(Test_directory):\n",
        "    os.mkdir(Test_directory)\n",
        "\n",
        "# Create training and validation splits\n",
        "Raw_directory = 'data/saved_crops_train'\n",
        "train_files = [f for f in sorted(glob.glob(Raw_directory+'/*.jpg'))]\n",
        "indices = np.arange(len(train_files))\n",
        "np.random.shuffle(indices)\n",
        "train_split, val_split, test_split = int(len(train_files)*0.7), int(len(train_files)*0.15), int(len(train_files)*0.15)\n",
        "indices = list(indices)\n",
        "train_imgs = [train_files[i] for i in indices[:train_split]]\n",
        "val_imgs = [train_files[i] for i in indices[train_split:train_split+val_split]]\n",
        "test_imgs = [train_files[i] for i in indices[train_split+val_split:]]\n",
        "\n",
        "# Copy images to different directories\n",
        "for i in range(len(train_imgs)):\n",
        "    shutil.copy(train_imgs[i], Train_directory+train_imgs[i].split('/')[-1].split('d_')[-1])\n",
        "for i in range(len(test_imgs)):\n",
        "    shutil.copy(test_imgs[i], Test_directory+test_imgs[i].split('/')[-1].split('d_')[-1])\n",
        "for i in range(len(val_imgs)):\n",
        "    shutil.copy(val_imgs[i], Val_directory+val_imgs[i].split('/')[-1].split('d_')[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target prcessing\n",
        "import pandas as pd\n",
        "train_ids = ['ISIC_'+train_imgs[i].split('_')[-1].split('.')[0] for i in range(len(train_imgs))]\n",
        "test_ids = ['ISIC_'+test_imgs[i].split('_')[-1].split('.')[0] for i in range(len(test_imgs))]\n",
        "val_ids = ['ISIC_'+val_imgs[i].split('_')[-1].split('.')[0] for i in range(len(val_imgs))]\n",
        "train_dataset = pd.read_csv('metadataTrain.csv')\n",
        "y = train_dataset[['ID','CLASS']]\n",
        "def get_y(train_ids, i):\n",
        "    a = y[y['ID'] == train_ids[i]]['CLASS'].values[0]\n",
        "    return a\n",
        "y_train = []\n",
        "for i in range(len(train_ids)):\n",
        "    y_train.append(get_y(train_ids, i))\n",
        "y_test = []\n",
        "for i in range(len(test_ids)):\n",
        "    y_test.append(get_y(test_ids, i))\n",
        "y_val = []\n",
        "for i in range(len(val_ids)):\n",
        "    y_val.append(get_y(val_ids, i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "BrYw9LK9W19-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_loader:  11916 val_loader:  2553 test_loader:  2554 submission_loader:  5677\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/albumentations/augmentations/transforms.py:1826: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from zmq import device\n",
        "import torch \n",
        "import albumentations as A \n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "Train_directory = 'data/Train_CNN/Train'\n",
        "Val_directory = 'data/Train_CNN/Val'\n",
        "Test_directory = 'data/Train_CNN/Test'\n",
        "Submission_directory = 'data/saved_crops_test'\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_WIDTH = 224\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "LEARNING_RATE = 1e-4\n",
        "train_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Rotate(limit=35, p=1.0),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.1),\n",
        "        A.RandomContrast(limit=0.6),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        # A.HueSaturationValue(val_shift_limit=50),\n",
        "        ToTensorV2(),\n",
        "\n",
        "    ],\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "\n",
        "    ],    \n",
        ")\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Normalize(\n",
        "            mean=[0.0, 0.0, 0.0],\n",
        "            std=[1.0, 1.0, 1.0],\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "\n",
        "    ],    \n",
        ")\n",
        "\n",
        "train_loader, val_loader, test_loader = get_loaders(\n",
        "    train_dir=Train_directory,\n",
        "    y_train=y_train,\n",
        "    val_dir=Val_directory,\n",
        "    y_val=y_val,\n",
        "    test_dir=Test_directory,\n",
        "    y_test=y_test,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    train_transform=train_transform,\n",
        "    val_transform=val_transform,\n",
        "    test_transform=test_transform,    \n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "submission_loader = get_test_loaders(\n",
        "    test_img_dir = Submission_directory,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    test_transform = test_transform,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        ")\n",
        "print('train_loader: ', len(train_loader.dataset), 'val_loader: ', len(val_loader.dataset),'test_loader: ', len(test_loader.dataset),'submission_loader: ', len(submission_loader.dataset) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-08 21:06:37.995235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:37.995261: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2022-05-08 21:06:40.877850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-05-08 21:06:40.878359: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.878553: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.878709: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.879037: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.879176: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.879322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.879462: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.879594: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/slimane/Documents/personal_docs/p_env/perso_env/lib/python3.8/site-packages/cv2/../../lib64:\n",
            "2022-05-08 21:06:40.879620: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2022-05-08 21:06:40.880633: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 33s 0us/step\n",
            "87924736/87910968 [==============================] - 33s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf \n",
        "IMG_SHAPE = (224,224,3)  # size of thumbnails used by the internal classification model\n",
        "CLASS_TO_INDEX = {\n",
        "            \"Negative\": 0,\n",
        "            \"Primordial\": 1,\n",
        "            \"Primary\": 2,\n",
        "            \"Secondary\": 3,\n",
        "            \"Tertiary\": 4,\n",
        "        }  # how to convert the provided classes of follicules to numbers\n",
        "INDEX_TO_CLASS = {value: key for key, value in CLASS_TO_INDEX.items()}\n",
        "# create classifier model based on MobileNet\n",
        "base_model = tf.keras.applications.InceptionV3(\n",
        "    input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\"\n",
        ")          \n",
        "#base_model = tf.keras.applications.MobileNetV2(\n",
        "#    input_shape=self.IMG_SHAPE, include_top=False, weights=\"imagenet\"\n",
        "#)\n",
        "base_model.trainable = True\n",
        "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n",
        "#preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "prediction_layer = tf.keras.layers.Dense(8, activation=\"softmax\")\n",
        "\n",
        "x = preprocess_input(inputs)\n",
        "#x = inputs\n",
        "x = base_model(x, training=False)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[\"sparse_categorical_accuracy\"],\n",
        ")\n",
        "_model = model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "tp_ima_205_cnn_part_1_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
